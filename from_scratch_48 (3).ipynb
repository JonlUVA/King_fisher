{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea55ff5a-5e53-42c2-97e9-0114d3713916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['model_weights', 'optimizer_weights']>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb601c-5990-42cc-a2ba-ef7eb5c4c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas\n",
    " \n",
    "od.download(\"https://www.kaggle.com/competitions/feather-in-focus/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbef826-114f-41b5-95d1-5b877b5e4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 11:29:35.350254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 11:29:35.543335: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/chekoduadarsh/starters-guide-convolutional-xgboost/notebook\n",
    "#https://www.kaggle.com/code/pedrolucasbritodes/bird-image-classification-cnn-89-accuracy/notebook\n",
    "#https://stats.stackexchange.com/questions/404809/is-it-advisable-to-use-output-from-a-ml-model-as-a-feature-in-another-ml-model\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "#how many epchos still revert\n",
    "patience1 = 1000\n",
    "\n",
    "# these are the decaying laying edit thing. think of a cool name \n",
    "patience2 = 4\n",
    "always_keep_percent_locked = 20\n",
    "start_unlocking_at = 80\n",
    "decay_by = 10\n",
    "\n",
    "blur_number = 4\n",
    "base_learning_rate = 0.001 \n",
    "#number before fine_tuning\n",
    "number_epcho1 = 20\n",
    "#number after fine_tuning\n",
    "number_epcho2 = 8\n",
    "number_class = 200\n",
    "#needs to be the same number of classes\n",
    "output_Neurons = 200\n",
    "Neurons = 512\n",
    "drop_out_rate = 0.4\n",
    "image_shape = (224,224)\n",
    "image_shape_full = (224,224,3)\n",
    "current_directory = \"feather-in-focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012c847-693a-4760-9cdd-f80a96740d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1ccf2b-4ac4-4cae-aac6-0b88a172b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2079.4624684/ipykernel_1423625/4078724003.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
      "/scratch-local/scur2079.4624684/ipykernel_1423625/4078724003.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>total_path</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/train_images/1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/1.jpg</td>\n",
       "      <td>1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/train_images/2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/2.jpg</td>\n",
       "      <td>2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/train_images/3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/3.jpg</td>\n",
       "      <td>3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/train_images/4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/4.jpg</td>\n",
       "      <td>4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/train_images/5.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/5.jpg</td>\n",
       "      <td>5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_path label  \\\n",
       "0  /train_images/1.jpg     1   \n",
       "1  /train_images/2.jpg     1   \n",
       "2  /train_images/3.jpg     1   \n",
       "3  /train_images/4.jpg     1   \n",
       "4  /train_images/5.jpg     1   \n",
       "\n",
       "                                         total_path short_name  \n",
       "0  feather-in-focus/train_images/train_images/1.jpg      1.jpg  \n",
       "1  feather-in-focus/train_images/train_images/2.jpg      2.jpg  \n",
       "2  feather-in-focus/train_images/train_images/3.jpg      3.jpg  \n",
       "3  feather-in-focus/train_images/train_images/4.jpg      4.jpg  \n",
       "4  feather-in-focus/train_images/train_images/5.jpg      5.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading files \n",
    "class_names2 = np.load('feather-in-focus/class_names.npy', allow_pickle=True).item()\n",
    "\n",
    "#load all files because why not\n",
    "test_image_path = pd.read_csv('feather-in-focus/test_images_path.csv')\n",
    "test_image_path['total_path'] = current_directory + \"/test_images\" + test_image_path['image_path'] \n",
    "test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "test_image = pd.read_csv('feather-in-focus/test_images_sample.csv')\n",
    "\n",
    "#importing training data \n",
    "train_image = pd.read_csv('feather-in-focus/train_images.csv')\n",
    "train_image['total_path'] = current_directory + \"/train_images\" + train_image['image_path']\n",
    "train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "train_image['label'] = train_image['label'].apply(str)\n",
    "train_image['short_name'] = train_image['image_path'].str.rsplit('/', n=1).str[-1]\n",
    "train_image.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c216ed-d088-45d8-a6f6-0d3e8f125e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add a k-folds, i have stratified so it works a bit better\n",
    "train, valid = train_test_split(train_image, \n",
    "                    test_size=0.2,\n",
    "                    stratify = train_image['label']\n",
    "                    #random_state=420\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20988c59-a2ec-4344-aa18-f3da469dab0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_aug\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace 'path_to_folder' with the actual path to your folder\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through all files in the folder\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[1;32m      7\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Check if the file is an image (you might want to refine this check based on your specific file types)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#deleting images if they are there \n",
    "\n",
    "folder_path = \"custom_aug\"  # Replace 'path_to_folder' with the actual path to your folder\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Check if the file is an image (you might want to refine this check based on your specific file types)\n",
    "    if os.path.isfile(file_path) and (filename.endswith('.jpg') or filename.endswith('.png')):\n",
    "        os.remove(file_path)\n",
    "        #print(f\"Deleted: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5787442a-4cfc-46f6-8b8f-b0f966a4ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom Augmentation \n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "persp_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "persp_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "gs_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "gs_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "blur_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "blur_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "def custom_blur(dataframe, label, name, location, folder,final_df, blur_number):\n",
    "    img = Image.open(dataframe[location]).filter(ImageFilter.GaussianBlur(blur_number))\n",
    "    img.save(r'{}/blur_{}'.format(folder,dataframe[name]))\n",
    "\n",
    "    return pd.DataFrame({\"image_path\": ['{}/blur_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/blur_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['blur_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "def custom_grey_scale(dataframe, label, name, location, folder,final_df):\n",
    "    img = Image.open(dataframe[location]).convert('L')\n",
    "    img.save(r'{}/gs_{}'.format(folder,dataframe[name]))\n",
    "\n",
    "    return pd.DataFrame({\"image_path\": ['{}/gs_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/gs_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['gs_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "def custom_getPerspectiveTransform(dataframe, label, name, location, folder,final_df):\n",
    "    \n",
    "    \n",
    "    input_image = cv2.imread(dataframe[location])  # Replace 'path_to_your_image.jpg' with the actual path to your image\n",
    "    # need to variability\n",
    "    height, width = input_image.shape[:2]\n",
    "      \n",
    "   # Define destination points based on image size (20% of width and 30% of height)\n",
    "    objPoints = np.float32([[0, 0], \n",
    "                            [int(np.random.uniform(0.75, 0.85) * width), 0], \n",
    "                            [0, int(np.random.uniform(0.85, 0.95) * height)], \n",
    "                            [int(np.random.uniform(0.75, 0.85) * width), \n",
    "                             int(np.random.uniform(0.65, 0.75) * height)]])\n",
    "    imgPts = np.float32([\n",
    "    [np.random.uniform(0.10, 0.20) * width, np.random.uniform(0.20, 0.30) * height],  # 15% of width, 25% of height\n",
    "    [np.random.uniform(0.75, 0.85) * width, np.random.uniform(0.15, 0.25) * height],  # 80% of width, 20% of height\n",
    "    [np.random.uniform(0.05, 0.15) * width, np.random.uniform(0.65, 0.75) * height],  # 10% of width, 70% of height\n",
    "    [np.random.uniform(0.85, 0.95) * width, np.random.uniform(0.65, 0.75) * height]   # 90% of width, 70% of height\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #imgPts = np.float32([[114, 151], [605, 89], [72, 420], [637, 420]])\n",
    "    # need to add varibility\n",
    "    #objPoints = np.float32([[0, 0], [420, 0], [0, 637], [420, 637]])\n",
    "    matrix = cv2.getPerspectiveTransform(imgPts, objPoints)\n",
    "    result = cv2.warpPerspective(input_image, matrix, (width, height))\n",
    "    \n",
    "    cv2.imwrite(r'{}/persp_{}'.format(folder,dataframe[name]), result)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame({\"image_path\": ['{}/persp_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/persp_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['persp_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "loc = \"custom_aug\" \n",
    "for index, row in train.iterrows():\n",
    "    persp_train_dataframe = pd.concat([persp_train_dataframe, \n",
    "           custom_getPerspectiveTransform(row, 'label', 'short_name', 'total_path', loc, persp_train_dataframe)], sort=False)\n",
    "    gs_train_dataframe = pd.concat([gs_train_dataframe, \n",
    "           custom_grey_scale(row, 'label', 'short_name', 'total_path', loc, gs_train_dataframe)], sort=False)\n",
    "    blur_train_dataframe = pd.concat([blur_train_dataframe, \n",
    "           custom_blur(row, 'label', 'short_name', 'total_path', loc, blur_train_dataframe,blur_number)], sort=False)\n",
    "    #print(row)\n",
    "train = pd.concat([train, persp_train_dataframe], sort=False)\n",
    "#train = pd.concat([train, gs_train_dataframe], sort=False)\n",
    "train = pd.concat([train, blur_train_dataframe], sort=False)\n",
    "\n",
    "for index, row in valid.iterrows():\n",
    "    persp_valid_dataframe = pd.concat([persp_valid_dataframe, \n",
    "               custom_getPerspectiveTransform(row, 'label', 'short_name', 'total_path', loc, persp_valid_dataframe)], sort=False)\n",
    "    gs_valid_dataframe = pd.concat([gs_valid_dataframe, \n",
    "          custom_grey_scale(row, 'label', 'short_name', 'total_path', loc, gs_valid_dataframe)], sort=False)\n",
    "    blur_valid_dataframe = pd.concat([blur_valid_dataframe, \n",
    "           custom_blur(row, 'label', 'short_name', 'total_path', loc, blur_valid_dataframe,blur_number)], sort=False)\n",
    "    #print(row)\n",
    "valid = pd.concat([valid, persp_valid_dataframe], sort=False)\n",
    "#valid = pd.concat([valid, gs_valid_dataframe], sort=False)\n",
    "valid = pd.concat([valid, blur_valid_dataframe], sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096a4b42-83f7-4327-bac6-b7682529da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_csv('valid_csv.csv')\n",
    "train.to_csv('train_csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e919614c-7114-4af7-ba92-18c63787fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>total_path</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1967</td>\n",
       "      <td>/train_images/1968.jpg</td>\n",
       "      <td>69</td>\n",
       "      <td>feather-in-focus/train_images/train_images/196...</td>\n",
       "      <td>1968.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2775</td>\n",
       "      <td>/train_images/2776.jpg</td>\n",
       "      <td>105</td>\n",
       "      <td>feather-in-focus/train_images/train_images/277...</td>\n",
       "      <td>2776.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>476</td>\n",
       "      <td>/train_images/477.jpg</td>\n",
       "      <td>16</td>\n",
       "      <td>feather-in-focus/train_images/train_images/477...</td>\n",
       "      <td>477.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3118</td>\n",
       "      <td>/train_images/3119.jpg</td>\n",
       "      <td>125</td>\n",
       "      <td>feather-in-focus/train_images/train_images/311...</td>\n",
       "      <td>3119.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3911</td>\n",
       "      <td>/train_images/3912.jpg</td>\n",
       "      <td>198</td>\n",
       "      <td>feather-in-focus/train_images/train_images/391...</td>\n",
       "      <td>3912.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9415</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_1480.jpg</td>\n",
       "      <td>50</td>\n",
       "      <td>custom_aug/blur_1480.jpg</td>\n",
       "      <td>blur_1480.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9416</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_2489.jpg</td>\n",
       "      <td>91</td>\n",
       "      <td>custom_aug/blur_2489.jpg</td>\n",
       "      <td>blur_2489.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9417</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_3312.jpg</td>\n",
       "      <td>137</td>\n",
       "      <td>custom_aug/blur_3312.jpg</td>\n",
       "      <td>blur_3312.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_2586.jpg</td>\n",
       "      <td>96</td>\n",
       "      <td>custom_aug/blur_2586.jpg</td>\n",
       "      <td>blur_2586.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_2003.jpg</td>\n",
       "      <td>70</td>\n",
       "      <td>custom_aug/blur_2003.jpg</td>\n",
       "      <td>blur_2003.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9420 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                image_path label  \\\n",
       "0           1967    /train_images/1968.jpg    69   \n",
       "1           2775    /train_images/2776.jpg   105   \n",
       "2            476     /train_images/477.jpg    16   \n",
       "3           3118    /train_images/3119.jpg   125   \n",
       "4           3911    /train_images/3912.jpg   198   \n",
       "...          ...                       ...   ...   \n",
       "9415           0  custom_aug/blur_1480.jpg    50   \n",
       "9416           0  custom_aug/blur_2489.jpg    91   \n",
       "9417           0  custom_aug/blur_3312.jpg   137   \n",
       "9418           0  custom_aug/blur_2586.jpg    96   \n",
       "9419           0  custom_aug/blur_2003.jpg    70   \n",
       "\n",
       "                                             total_path     short_name  \n",
       "0     feather-in-focus/train_images/train_images/196...       1968.jpg  \n",
       "1     feather-in-focus/train_images/train_images/277...       2776.jpg  \n",
       "2     feather-in-focus/train_images/train_images/477...        477.jpg  \n",
       "3     feather-in-focus/train_images/train_images/311...       3119.jpg  \n",
       "4     feather-in-focus/train_images/train_images/391...       3912.jpg  \n",
       "...                                                 ...            ...  \n",
       "9415                           custom_aug/blur_1480.jpg  blur_1480.jpg  \n",
       "9416                           custom_aug/blur_2489.jpg  blur_2489.jpg  \n",
       "9417                           custom_aug/blur_3312.jpg  blur_3312.jpg  \n",
       "9418                           custom_aug/blur_2586.jpg  blur_2586.jpg  \n",
       "9419                           custom_aug/blur_2003.jpg  blur_2003.jpg  \n",
       "\n",
       "[9420 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid = pd.read_csv('valid_csv.csv')\n",
    "train = pd.read_csv('train_csv.csv')\n",
    "valid['label'] = valid['label'].astype(str)\n",
    "train['label'] = train['label'].astype(str)\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b8bb47-9f6c-4815-b420-60f7e3d321fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9420 validated image filenames belonging to 200 classes.\n",
      "Found 2358 validated image filenames belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "# this creates the type of augmentation that is done! \n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 15,\n",
    "    horizontal_flip = True,\n",
    "    zoom_range = 0.20)\n",
    "#look at aug from group chat \n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#datagen = ImageDataGenerator(\n",
    "#    rotation_range=20,            # Reducing the rotation range for birds\n",
    "#    width_shift_range=0.1,\n",
    "#    height_shift_range=0.1,\n",
    "#    shear_range=10,               # Modifying the shear range\n",
    "#    horizontal_flip=True,         # Birds might benefit from horizontal flip\n",
    "#    zoom_range=0.15)              # Slight zooming for variation\n",
    "\n",
    "# Optionally, you can add more specific augmentation techniques such as brightness adjustment,\n",
    "# contrast enhancement, or any other relevant transformations that might suit bird images.\n",
    "\n",
    "\n",
    "#okay this method seems WAY better. only ever use aug images\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = train['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=valid,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = valid['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42ea244-aa84-47b7-b1c4-ed4298ce6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_data = pd.DataFrame({'real_label': np.unique(train_image['label']), \n",
    "                           'keras_label':pd.DataFrame(np.unique(train_image['label'])).index})\n",
    "class_weights = train_image.merge(small_data, left_on = 'label', right_on = 'real_label')\n",
    "class_number = class_weights['keras_label']\n",
    "#class_number = class_weights['real_label'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bae1a-886b-4a97-b043-bfd6a88e3568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416049e2-4547-49fb-999a-fcfdb4640f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(class_number),\n",
    "                                        y = class_number                                                    \n",
    "                                    )\n",
    "\n",
    "class_weights = dict(zip(np.unique(class_number), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4e3af-a22b-43d3-9834-e51721639c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20014454-8152-4714-83c6-6aa6bde1cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert Keras symbolic inputs/outputs to numpy arrays\n",
    "def convert_to_numpy(arr):\n",
    "    return arr.numpy() if hasattr(arr, 'numpy') else arr\n",
    "\n",
    "# Modify the calculate_distances function to handle numpy arrays or lists\n",
    "def calculate_distances(classes, lookup):\n",
    "    classes = [convert_to_numpy(item) for item in classes]\n",
    "    result = [[classes[0][i], classes[1][i], classes[2][i]] for i in 200]\n",
    "    distances = []\n",
    "    for arr1 in result:\n",
    "        distances_to_arr1 = []\n",
    "        for arr2 in lookup:\n",
    "            # Calculate Euclidean distance between elements in arr1 and arr2\n",
    "            distance = np.linalg.norm(np.array(arr1) - np.array(arr2))\n",
    "            distances_to_arr1.append(distance)\n",
    "        distances.append(distances_to_arr1)\n",
    "    return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5777f519-030f-4e53-96d8-7ad95ff239b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 11:30:16.907495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 11:30:17.331196: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-12-11 11:30:17.331507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38372 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:32:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " multi_output_bird_net (Functio  [(None, 1),         65357991    ['input_3[0][0]']                \n",
      " nal)                            (None, 1),                                                       \n",
      "                                 (None, 1),                                                       \n",
      "                                 (None, 4)]                                                       \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 7)            0           ['multi_output_bird_net[0][0]',  \n",
      "                                                                  'multi_output_bird_net[0][1]',  \n",
      "                                                                  'multi_output_bird_net[0][2]',  \n",
      "                                                                  'multi_output_bird_net[0][3]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          4096        ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " resnet50 (Functional)          (None, 2048)         23587712    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)      (None, 2048)         21802784    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " xception (Functional)          (None, 2048)         20861480    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 6656)         0           ['resnet50[0][0]',               \n",
      "                                                                  'inception_v3[0][0]',           \n",
      "                                                                  'dropout[0][0]',                \n",
      "                                                                  'xception[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          3408384     ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 512)         2048        ['dense_1[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 200)          102600      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135,127,095\n",
      "Trainable params: 35,051,599\n",
      "Non-trainable params: 100,075,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "M1 = 'Tarsus.Length'\n",
    "M2 = 'Hand-Wing.Index'\n",
    "M3 = 'Beak.Depth'\n",
    "c1 = 'Primary.Lifestyle'\n",
    "# load the trained model\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model_path = '{}/keras_final_continous.keras'.format(current_directory)\n",
    "continous = load_model(model_path)\n",
    "continous.compile(optimizer=adam_optimizer,\n",
    "              loss={\n",
    "                  '{}_output'.format(M1): 'mse',\n",
    "                  '{}_output'.format(M2): 'mse',\n",
    "                  '{}_output'.format(M3): 'mse',\n",
    "                '{}_output'.format(c1):  \"categorical_crossentropy\"\n",
    "                  \n",
    "              },\n",
    "              metrics={\n",
    "                  '{}_output'.format(M1): 'mae',\n",
    "                  '{}_output'.format(M2): 'mae',\n",
    "                  '{}_output'.format(M3): 'mae',\n",
    "                  '{}_output'.format(c1):  \"accuracy\"\n",
    "              })\n",
    "\n",
    "\n",
    "# Create the base models (ResNet50 and InceptionV3)\n",
    "base_model_ResNet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    #classes = 200,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "base_model_InceptionV3 = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    #classes = 200,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "#for layers in continous:\n",
    "#    layers.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Freeze most layers but unfreeze the last ones for fine-tuning\n",
    "for layer in base_model_ResNet50.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model_InceptionV3.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the input layer\n",
    "inputs = tf.keras.Input(shape=image_shape_full)\n",
    "\n",
    "\n",
    "# Create the base models (ResNet50 and InceptionV3)\n",
    "base_model_NASN = tf.keras.applications.NASNetLarge(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    #classes = 200,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze most layers but unfreeze the last ones for fine-tuning\n",
    "for layer in base_model_NASN.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "x4 = base_model_NASN(inputs, training=False)\n",
    "\n",
    "\n",
    "\n",
    "base_model_Xception = tf.keras.applications.Xception(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    #classes = 200,\n",
    "    weights='imagenet'\n",
    ")\n",
    "for layer in base_model_Xception.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Freeze most layers but unfreeze the last ones for fine-tuning\n",
    "for layer in base_model_Xception.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "x5 = base_model_Xception(inputs, training=False)\n",
    "\n",
    "\n",
    "base_model_VGG16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    #classes = 200,\n",
    "    weights='imagenet'\n",
    ")\n",
    "for layer in base_model_VGG16.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "x6 = base_model_VGG16(inputs, training=False)\n",
    "\n",
    "\n",
    "# Pass the input through the base models\n",
    "x1 = base_model_ResNet50(inputs, training=False)\n",
    "x2 = base_model_InceptionV3(inputs, training=False)\n",
    "x3 = continous(inputs, training = False)\n",
    "x3 = tf.keras.layers.Concatenate()(x3)\n",
    "\n",
    "x3 = tf.keras.layers.Dense(512, activation=\"relu\")(x3)\n",
    "x3 = tf.keras.layers.Dropout(0.4)(x3)\n",
    "\n",
    "#result_distances = calculate_distances(x3, look_up_continous_numpy)\n",
    "#x1 is resnet\n",
    "#x2 is inceptionv3\n",
    "#x3 is continous\n",
    "#4 NASN\n",
    "# x5 inception\n",
    "\n",
    "# Merge the features from both base models\n",
    "combined_features = tf.keras.layers.Concatenate()([x1, \n",
    "                                                   x2,\n",
    "                                                   x3, \n",
    "                                                   #x4, \n",
    "                                                   x5,\n",
    "                                                   x6\n",
    "                                                  ])\n",
    "\n",
    "\n",
    "\n",
    "# Add dense layers with dropout and batch normalization\n",
    "x = tf.keras.layers.Dense(Neurons, activation=\"relu\")(combined_features)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = tf.keras.layers.Dense(output_Neurons, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Adam optimizer\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "\n",
    "# Compile the model \n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07aea2-099c-4e6c-b825-135f04753f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 11:31:14.690693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-12-11 11:31:18.330977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-11 11:31:18.494177: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x27517100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-11 11:31:18.494205: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2023-12-11 11:31:18.523461: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-11 11:31:18.784409: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 182s 477ms/step - loss: 5.9797 - accuracy: 0.0089 - val_loss: 20.3321 - val_accuracy: 0.0153 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "295/295 [==============================] - 120s 407ms/step - loss: 5.1498 - accuracy: 0.0151 - val_loss: 4.8987 - val_accuracy: 0.0237 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "295/295 [==============================] - 119s 403ms/step - loss: 4.8433 - accuracy: 0.0192 - val_loss: 4.8783 - val_accuracy: 0.0216 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 4.7095 - accuracy: 0.0266 - val_loss: 2784.2878 - val_accuracy: 0.0280 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "295/295 [==============================] - 119s 404ms/step - loss: 4.4963 - accuracy: 0.0406 - val_loss: 4.4775 - val_accuracy: 0.0424 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 4.2079 - accuracy: 0.0659 - val_loss: 4.6413 - val_accuracy: 0.0483 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "295/295 [==============================] - 119s 403ms/step - loss: 3.9717 - accuracy: 0.0945 - val_loss: 4.2658 - val_accuracy: 0.0789 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "295/295 [==============================] - 119s 405ms/step - loss: 3.7540 - accuracy: 0.1137 - val_loss: 4.1702 - val_accuracy: 0.0848 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 3.5691 - accuracy: 0.1418 - val_loss: 4.1704 - val_accuracy: 0.1001 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "295/295 [==============================] - 119s 403ms/step - loss: 3.3924 - accuracy: 0.1706 - val_loss: 4.0786 - val_accuracy: 0.1170 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 3.2273 - accuracy: 0.1931 - val_loss: 859.9817 - val_accuracy: 0.1009 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 3.0551 - accuracy: 0.2282 - val_loss: 3.5970 - val_accuracy: 0.1798 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "295/295 [==============================] - 119s 405ms/step - loss: 2.9276 - accuracy: 0.2522 - val_loss: 3.4800 - val_accuracy: 0.1836 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 2.7992 - accuracy: 0.2807 - val_loss: 3.4904 - val_accuracy: 0.2078 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "295/295 [==============================] - 119s 404ms/step - loss: 2.7010 - accuracy: 0.3006 - val_loss: 1309.3055 - val_accuracy: 0.2362 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "295/295 [==============================] - 119s 404ms/step - loss: 2.5992 - accuracy: 0.3234 - val_loss: 3.3545 - val_accuracy: 0.2265 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 2.4913 - accuracy: 0.3396 - val_loss: 3.8017 - val_accuracy: 0.2193 - lr: 0.0100\n",
      "Epoch 18/20\n",
      " 85/295 [=======>......................] - ETA: 1:07 - loss: 2.3755 - accuracy: 0.3743"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Setting the early_stop to avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=patience1,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,  # Specify the 'patience' parameter here\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=number_epcho1,\n",
    "    batch_size=52,\n",
    "    validation_data=valid_generator,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce402a-c6c2-4e0a-9f4d-137161dc61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('result.png')\n",
    "plt.clf() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdccb1-5cc7-4bbb-81dd-d8c2a98630e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f154a81-fc5e-4672-bcdc-fbab118a1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 80\n",
    "# Now that we will allow some layers to be unfreezed, it's better to decrease the learning rate to avoid dramatic changes in those \n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/100)\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(\n",
    "    #loss = \"sparse_categorical_crossentropy\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=patience2,\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True,)\n",
    "\n",
    "total_epochs = number_epcho1 + number_epcho2\n",
    "history_fine = model.fit(\n",
    "                        train_generator,\n",
    "                         epochs=total_epochs,\n",
    "                         batch_size = 32,\n",
    "                         initial_epoch=20,\n",
    "                         steps_per_epoch=len(train_generator),\n",
    "                         validation_data=valid_generator,\n",
    "                         #validation_steps=int(0.25 * len(valid_generator)),\n",
    "                        callbacks=[early_stop, reduce_lr],\n",
    "    #class_weight=class_weights\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339a678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71b72e-2c58-43e0-a77c-1873558068f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6096899-bb39-476c-a66d-6516687e4d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4a266-715a-4bd0-b6f5-9708bd635623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history_fine.history['accuracy'])\n",
    "plt.plot(history_fine.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37765a38-cbcc-4ae5-bc40-b3fdfc22184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model with the new name\n",
    "#model.save('feather-in-focus/keras_final4.h5')\n",
    "# image folder\n",
    "#folder_path = '{}/test_images/test_images/'.format(current_directory)\n",
    "# path to model\n",
    "#model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "# dimensions of images\n",
    "#img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e75886-03d7-4aff-a1f7-dddec3f5c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09f44c-5270-4428-aa43-03f0a9b6726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "import keras.utils as image\n",
    "from tensorflow.keras.models import load_model\n",
    "# load the trained model\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "#model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "#model_path = 'feather-in-focus/keras_final4.h5'\n",
    "#model = load_model(model_path)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb8c1a-3551-4bb8-9440-2a311384c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list()\n",
    "images = []\n",
    "folder_path = 'feather-in-focus/test_images/test_images'  # Your folder path containing images\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    # Check if the item is a file (not a directory)\n",
    "    if os.path.isfile(img_path):\n",
    "        name.append(img_name)\n",
    "        img = image.load_img(img_path, target_size=image_shape)\n",
    "        img = image.img_to_array(img)\n",
    "        \n",
    "        \n",
    "        #img_array = image.img_to_array(img)\n",
    "        #img = img.astype('float32') / 255.0  # Normalize pixel values\n",
    "\n",
    "# Reshape the image to a 4D tensor with a single sample (to match model input shape)\n",
    "        #img = img.reshape((1,) + img.shape)\n",
    "        \n",
    "        \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "        # Process the image further as needed\n",
    "    else:\n",
    "        # Skip directories\n",
    "        continue\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d812d7f-e48e-473d-9aa4-38697b8edc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea6649-cac1-4b50-ac15-8f755db542ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = model.predict(images, batch_size=10)\n",
    "classes_x=np.argmax(classes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311fc31-09b1-4c2c-a8cd-0a740a4a1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(\n",
    "    {'id': name,\n",
    "     'label_predict': classes_x\n",
    "    })\n",
    "\n",
    "final_df['id2'] = \"/test_images/\"+final_df['id']\n",
    "final_df['label_predict'] = final_df['label_predict'].astype(int)\n",
    "final_df = final_df[['label_predict','id2']]\n",
    "\n",
    "idea = np.sort(train_image['label'].unique())\n",
    "idea2 = pd.DataFrame(idea)\n",
    "idea2['index1'] = idea2.index\n",
    "idea2.columns = ['values', 'index1']\n",
    "idea2['values'] = idea2['values'].astype(int)\n",
    "\n",
    "final_df4 = final_df.merge(test_image_path[['image_path','id']], left_on = 'id2', right_on='image_path')\n",
    "\n",
    "final_df4 = final_df4[['id','label_predict']]\n",
    "\n",
    "final_df2 = final_df4.merge(idea2, how = 'left', left_on = 'label_predict', right_on = 'index1')\n",
    "final_df3 = final_df2[['id','values']]\n",
    "#two issues the class values arent right\n",
    "final_df3.columns = ['id','label']\n",
    "final_df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a672dc-cc4a-4290-b23e-674f2771c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df3.to_csv('output_folder/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "885c943d-aa26-497a-b061-90c6089afd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset metadata saved to output_folder/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the metadata for your dataset\n",
    "metadata = {\n",
    "    \"title\": \"output.csv\",\n",
    "    \"id\": \"uvajonathon\",\n",
    "    \"licenses\": [\n",
    "        {\n",
    "            \"name\": \"License Name\",\n",
    "            \"id\": \"uvajonathon\",\n",
    "            \"url\": \"URL to License if applicable\"\n",
    "        }\n",
    "    ],\n",
    "    \"description\": \"Description of your dataset\",\n",
    "    \"keywords\": [\"keyword1\", \"keyword2\"],\n",
    "    \"language\": \"en\",\n",
    "    \"isPrivate\": False,  # Set to True or False based on privacy\n",
    "    \"collaborators\": [\"uvajonathon\"],\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"description\": \"Description of the data\",\n",
    "            \"name\": \"output.csv\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the metadata to dataset-metadata.json\n",
    "file_path = 'output_folder/dataset-metadata.json'  # Specify the path where you want to save the file\n",
    "\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Dataset metadata saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b923057-24fc-4014-bbde-884e2411a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# Set the path to your Kaggle API credentials file (kaggle.json)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/gpfs/home2/scur2079/'\n",
    "# Instantiate the Kaggle API\n",
    "api = KaggleApi()\n",
    "# Replace 'competition-name' with the competition name (as shown in the competition URL)\n",
    "competition_name = 'feather-in-focus'\n",
    "# Replace 'file-path.csv' with the path to your CSV file\n",
    "file_path = '/gpfs/home2/scur2079/output_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ba3c9-b563-4074-8edd-5286f9103546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the competition to which you want to upload the file\n",
    "api.competition_name = competition_name\n",
    "# Upload the CSV file to the competition\n",
    "api.dataset_create_new(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c890d-a845-434b-9afd-33e921d88e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b78d1a-9fdc-4412-948b-68de182a2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your Keras model as described in your provided code\n",
    "\n",
    "# Assuming you've trained your Keras model and stored it in a variable 'model'\n",
    "from keras.models import Model\n",
    "# Extract features using the Keras Functional API\n",
    "# Create a new model that takes the same input as the previous model but outputs the output of the desired layer\n",
    "layer_name='output-layer'\n",
    "feature_extraction_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "# \n",
    "# Get features for training and validation data\n",
    "train_features = feature_extraction_model.predict(train_generator)  # Replace train_data with your actual training data\n",
    "#valid_features = feature_extraction_model.predict(valid)  # Replace valid_data with your actual validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072233a-0e01-4de0-acb0-b52769386e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841d9b1-1acb-4592-9fcf-86c39b74cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(image, label):\n",
    "    return label\n",
    "\n",
    "# Iterate through the generator to extract labels\n",
    "labels = []\n",
    "for images, labels_batch in train_generator:\n",
    "    labels.extend(extract_labels(images, labels_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd146b7-8457-4984-a4a9-0282f9af33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfd7fd-0162-494a-b51f-83bb2e8d5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()  # You can set hyperparameters as needed\n",
    "xgb_model.fit(train_features,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238ea9e-6c0b-4f97-a793-49c57341cc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52c494-620a-4e15-8d10-ea63c50687e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list()\n",
    "images = []\n",
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "folder_path = 'feather-in-focus/test_images/test_images'  # Your folder path containing images\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    # Check if the item is a file (not a directory)\n",
    "    if os.path.isfile(img_path):\n",
    "        name.append(img_name)\n",
    "        img = image.load_img(img_path, target_size=image_shape)\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "        # Process the image further as needed\n",
    "    else:\n",
    "        # Skip directories\n",
    "        continue\n",
    "        \n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = feature_extraction_model.predict(images, batch_size=10)\n",
    "classes_x=np.argmax(classes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49b730-4777-4244-8922-1ce619171d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "valid_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c99bb-1486-45ea-93b4-9641014fe3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_x=np.argmax(valid_features,axis=1)\n",
    "final_df = pd.DataFrame(\n",
    "    {'id': name,\n",
    "     'label_predict': classes_x\n",
    "    })\n",
    "\n",
    "final_df['id2'] = \"/test_images/\"+final_df['id']\n",
    "final_df['label_predict'] = final_df['label_predict'].astype(int)\n",
    "final_df = final_df[['label_predict','id2']]\n",
    "\n",
    "idea = np.sort(train_image['label'].unique())\n",
    "idea2 = pd.DataFrame(idea)\n",
    "idea2['index1'] = idea2.index\n",
    "idea2.columns = ['values', 'index1']\n",
    "idea2['values'] = idea2['values'].astype(int)\n",
    "\n",
    "final_df4 = final_df.merge(test_image_path[['image_path','id']], left_on = 'id2', right_on='image_path')\n",
    "\n",
    "final_df4 = final_df4[['id','label_predict']]\n",
    "\n",
    "final_df2 = final_df4.merge(idea2, how = 'left', left_on = 'label_predict', right_on = 'index1')\n",
    "final_df3 = final_df2[['id','values']]\n",
    "#two issues the class values arent right\n",
    "final_df3.columns = ['id','label']\n",
    "final_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fc304-f43f-4c3a-b75f-445051a1ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
