{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55ff5a-5e53-42c2-97e9-0114d3713916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabb601c-5990-42cc-a2ba-ef7eb5c4c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./feather-in-focus\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import pandas\n",
    " \n",
    "od.download(\"https://www.kaggle.com/competitions/feather-in-focus/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbef826-114f-41b5-95d1-5b877b5e4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 16:38:31.597635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-08 16:38:31.951750: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/chekoduadarsh/starters-guide-convolutional-xgboost/notebook\n",
    "#https://www.kaggle.com/code/pedrolucasbritodes/bird-image-classification-cnn-89-accuracy/notebook\n",
    "#https://stats.stackexchange.com/questions/404809/is-it-advisable-to-use-output-from-a-ml-model-as-a-feature-in-another-ml-model\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "#how many epchos still revert\n",
    "patience1 = 1000\n",
    "\n",
    "# these are the decaying laying edit thing. think of a cool name \n",
    "patience2 = 4\n",
    "always_keep_percent_locked = 20\n",
    "start_unlocking_at = 80\n",
    "decay_by = 10\n",
    "\n",
    "blur_number = 4\n",
    "base_learning_rate = 0.001 \n",
    "#number before fine_tuning\n",
    "number_epcho1 = 50\n",
    "#number after fine_tuning\n",
    "number_epcho2 = 30\n",
    "number_class = 200\n",
    "#needs to be the same number of classes\n",
    "output_Neurons = 200\n",
    "Neurons = 512\n",
    "drop_out_rate = 0.3\n",
    "image_shape = (224,224)\n",
    "image_shape_full = (224,224,3)\n",
    "current_directory = \"feather-in-focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012c847-693a-4760-9cdd-f80a96740d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ccf2b-4ac4-4cae-aac6-0b88a172b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2079.4612880/ipykernel_2321061/4078724003.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
      "/scratch-local/scur2079.4612880/ipykernel_2321061/4078724003.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>total_path</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/train_images/1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/1.jpg</td>\n",
       "      <td>1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/train_images/2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/2.jpg</td>\n",
       "      <td>2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/train_images/3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/3.jpg</td>\n",
       "      <td>3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/train_images/4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/4.jpg</td>\n",
       "      <td>4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/train_images/5.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/5.jpg</td>\n",
       "      <td>5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_path label  \\\n",
       "0  /train_images/1.jpg     1   \n",
       "1  /train_images/2.jpg     1   \n",
       "2  /train_images/3.jpg     1   \n",
       "3  /train_images/4.jpg     1   \n",
       "4  /train_images/5.jpg     1   \n",
       "\n",
       "                                         total_path short_name  \n",
       "0  feather-in-focus/train_images/train_images/1.jpg      1.jpg  \n",
       "1  feather-in-focus/train_images/train_images/2.jpg      2.jpg  \n",
       "2  feather-in-focus/train_images/train_images/3.jpg      3.jpg  \n",
       "3  feather-in-focus/train_images/train_images/4.jpg      4.jpg  \n",
       "4  feather-in-focus/train_images/train_images/5.jpg      5.jpg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading files \n",
    "class_names2 = np.load('feather-in-focus/class_names.npy', allow_pickle=True).item()\n",
    "\n",
    "#load all files because why not\n",
    "test_image_path = pd.read_csv('feather-in-focus/test_images_path.csv')\n",
    "test_image_path['total_path'] = current_directory + \"/test_images\" + test_image_path['image_path'] \n",
    "test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "test_image = pd.read_csv('feather-in-focus/test_images_sample.csv')\n",
    "\n",
    "#importing training data \n",
    "train_image = pd.read_csv('feather-in-focus/train_images.csv')\n",
    "train_image['total_path'] = current_directory + \"/train_images\" + train_image['image_path']\n",
    "train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "train_image['label'] = train_image['label'].apply(str)\n",
    "train_image['short_name'] = train_image['image_path'].str.rsplit('/', n=1).str[-1]\n",
    "train_image.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c216ed-d088-45d8-a6f6-0d3e8f125e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add a k-folds, i have stratified so it works a bit better\n",
    "train, valid = train_test_split(train_image, \n",
    "                    test_size=0.2,\n",
    "                    stratify = train_image['label']\n",
    "                    #random_state=420\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20988c59-a2ec-4344-aa18-f3da469dab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting images if they are there \n",
    "\n",
    "folder_path = \"custom_aug\"  # Replace 'path_to_folder' with the actual path to your folder\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Check if the file is an image (you might want to refine this check based on your specific file types)\n",
    "    if os.path.isfile(file_path) and (filename.endswith('.jpg') or filename.endswith('.png')):\n",
    "        os.remove(file_path)\n",
    "        #print(f\"Deleted: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5787442a-4cfc-46f6-8b8f-b0f966a4ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom Augmentation \n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "persp_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "persp_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "gs_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "gs_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "blur_train_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "blur_valid_dataframe = pd.DataFrame(columns=['image_path','label','total_path','short_name'])\n",
    "\n",
    "def custom_blur(dataframe, label, name, location, folder,final_df, blur_number):\n",
    "    img = Image.open(dataframe[location]).filter(ImageFilter.GaussianBlur(blur_number))\n",
    "    img.save(r'{}/blur_{}'.format(folder,dataframe[name]))\n",
    "\n",
    "    return pd.DataFrame({\"image_path\": ['{}/blur_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/blur_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['blur_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "def custom_grey_scale(dataframe, label, name, location, folder,final_df):\n",
    "    img = Image.open(dataframe[location]).convert('L')\n",
    "    img.save(r'{}/gs_{}'.format(folder,dataframe[name]))\n",
    "\n",
    "    return pd.DataFrame({\"image_path\": ['{}/gs_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/gs_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['gs_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "def custom_getPerspectiveTransform(dataframe, label, name, location, folder,final_df):\n",
    "    \n",
    "    \n",
    "    input_image = cv2.imread(dataframe[location])  # Replace 'path_to_your_image.jpg' with the actual path to your image\n",
    "    # need to variability\n",
    "    height, width = input_image.shape[:2]\n",
    "      \n",
    "   # Define destination points based on image size (20% of width and 30% of height)\n",
    "    objPoints = np.float32([[0, 0], \n",
    "                            [int(np.random.uniform(0.75, 0.85) * width), 0], \n",
    "                            [0, int(np.random.uniform(0.85, 0.95) * height)], \n",
    "                            [int(np.random.uniform(0.75, 0.85) * width), \n",
    "                             int(np.random.uniform(0.65, 0.75) * height)]])\n",
    "    imgPts = np.float32([\n",
    "    [np.random.uniform(0.10, 0.20) * width, np.random.uniform(0.20, 0.30) * height],  # 15% of width, 25% of height\n",
    "    [np.random.uniform(0.75, 0.85) * width, np.random.uniform(0.15, 0.25) * height],  # 80% of width, 20% of height\n",
    "    [np.random.uniform(0.05, 0.15) * width, np.random.uniform(0.65, 0.75) * height],  # 10% of width, 70% of height\n",
    "    [np.random.uniform(0.85, 0.95) * width, np.random.uniform(0.65, 0.75) * height]   # 90% of width, 70% of height\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #imgPts = np.float32([[114, 151], [605, 89], [72, 420], [637, 420]])\n",
    "    # need to add varibility\n",
    "    #objPoints = np.float32([[0, 0], [420, 0], [0, 637], [420, 637]])\n",
    "    matrix = cv2.getPerspectiveTransform(imgPts, objPoints)\n",
    "    result = cv2.warpPerspective(input_image, matrix, (width, height))\n",
    "    \n",
    "    cv2.imwrite(r'{}/persp_{}'.format(folder,dataframe[name]), result)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame({\"image_path\": ['{}/persp_{}'.format(folder,dataframe[name])],\n",
    "                      'label': [dataframe[label]],\n",
    "                      'total_path': ['{}/persp_{}'.format(folder,dataframe[name])],\n",
    "                      'short_name': ['persp_{}'.format(dataframe[name])]\n",
    "                      })\n",
    "\n",
    "loc = \"custom_aug\" \n",
    "for index, row in train.iterrows():\n",
    "    persp_train_dataframe = pd.concat([persp_train_dataframe, \n",
    "           custom_getPerspectiveTransform(row, 'label', 'short_name', 'total_path', loc, persp_train_dataframe)], sort=False)\n",
    "    gs_train_dataframe = pd.concat([gs_train_dataframe, \n",
    "           custom_grey_scale(row, 'label', 'short_name', 'total_path', loc, gs_train_dataframe)], sort=False)\n",
    "    blur_train_dataframe = pd.concat([blur_train_dataframe, \n",
    "           custom_blur(row, 'label', 'short_name', 'total_path', loc, blur_train_dataframe,blur_number)], sort=False)\n",
    "    #print(row)\n",
    "train = pd.concat([train, persp_train_dataframe], sort=False)\n",
    "#train = pd.concat([train, gs_train_dataframe], sort=False)\n",
    "train = pd.concat([train, blur_train_dataframe], sort=False)\n",
    "\n",
    "for index, row in valid.iterrows():\n",
    "    persp_valid_dataframe = pd.concat([persp_valid_dataframe, \n",
    "               custom_getPerspectiveTransform(row, 'label', 'short_name', 'total_path', loc, persp_valid_dataframe)], sort=False)\n",
    "    #gs_valid_dataframe = pd.concat([gs_valid_dataframe, \n",
    "           #custom_grey_scale(row, 'label', 'short_name', 'total_path', loc, gs_valid_dataframe)], sort=False)\n",
    "    blur_valid_dataframe = pd.concat([blur_valid_dataframe, \n",
    "           custom_blur(row, 'label', 'short_name', 'total_path', loc, blur_valid_dataframe,blur_number)], sort=False)\n",
    "    #print(row)\n",
    "valid = pd.concat([valid, persp_valid_dataframe], sort=False)\n",
    "#valid = pd.concat([valid, gs_valid_dataframe], sort=False)\n",
    "valid = pd.concat([valid, blur_valid_dataframe], sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096a4b42-83f7-4327-bac6-b7682529da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_csv('valid_csv.csv')\n",
    "train.to_csv('train_csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e919614c-7114-4af7-ba92-18c63787fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>total_path</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3653</td>\n",
       "      <td>/train_images/3654.jpg</td>\n",
       "      <td>165</td>\n",
       "      <td>feather-in-focus/train_images/train_images/365...</td>\n",
       "      <td>3654.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2528</td>\n",
       "      <td>/train_images/2529.jpg</td>\n",
       "      <td>93</td>\n",
       "      <td>feather-in-focus/train_images/train_images/252...</td>\n",
       "      <td>2529.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992</td>\n",
       "      <td>/train_images/1993.jpg</td>\n",
       "      <td>70</td>\n",
       "      <td>feather-in-focus/train_images/train_images/199...</td>\n",
       "      <td>1993.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2622</td>\n",
       "      <td>/train_images/2623.jpg</td>\n",
       "      <td>98</td>\n",
       "      <td>feather-in-focus/train_images/train_images/262...</td>\n",
       "      <td>2623.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3568</td>\n",
       "      <td>/train_images/3569.jpg</td>\n",
       "      <td>157</td>\n",
       "      <td>feather-in-focus/train_images/train_images/356...</td>\n",
       "      <td>3569.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9415</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_546.jpg</td>\n",
       "      <td>19</td>\n",
       "      <td>custom_aug/blur_546.jpg</td>\n",
       "      <td>blur_546.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9416</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_608.jpg</td>\n",
       "      <td>21</td>\n",
       "      <td>custom_aug/blur_608.jpg</td>\n",
       "      <td>blur_608.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9417</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_3875.jpg</td>\n",
       "      <td>191</td>\n",
       "      <td>custom_aug/blur_3875.jpg</td>\n",
       "      <td>blur_3875.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_1233.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>custom_aug/blur_1233.jpg</td>\n",
       "      <td>blur_1233.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>0</td>\n",
       "      <td>custom_aug/blur_3572.jpg</td>\n",
       "      <td>157</td>\n",
       "      <td>custom_aug/blur_3572.jpg</td>\n",
       "      <td>blur_3572.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                image_path label  \\\n",
       "0           3653    /train_images/3654.jpg   165   \n",
       "1           2528    /train_images/2529.jpg    93   \n",
       "2           1992    /train_images/1993.jpg    70   \n",
       "3           2622    /train_images/2623.jpg    98   \n",
       "4           3568    /train_images/3569.jpg   157   \n",
       "...          ...                       ...   ...   \n",
       "9415           0   custom_aug/blur_546.jpg    19   \n",
       "9416           0   custom_aug/blur_608.jpg    21   \n",
       "9417           0  custom_aug/blur_3875.jpg   191   \n",
       "9418           0  custom_aug/blur_1233.jpg    41   \n",
       "9419           0  custom_aug/blur_3572.jpg   157   \n",
       "\n",
       "                                             total_path     short_name  \n",
       "0     feather-in-focus/train_images/train_images/365...       3654.jpg  \n",
       "1     feather-in-focus/train_images/train_images/252...       2529.jpg  \n",
       "2     feather-in-focus/train_images/train_images/199...       1993.jpg  \n",
       "3     feather-in-focus/train_images/train_images/262...       2623.jpg  \n",
       "4     feather-in-focus/train_images/train_images/356...       3569.jpg  \n",
       "...                                                 ...            ...  \n",
       "9415                            custom_aug/blur_546.jpg   blur_546.jpg  \n",
       "9416                            custom_aug/blur_608.jpg   blur_608.jpg  \n",
       "9417                           custom_aug/blur_3875.jpg  blur_3875.jpg  \n",
       "9418                           custom_aug/blur_1233.jpg  blur_1233.jpg  \n",
       "9419                           custom_aug/blur_3572.jpg  blur_3572.jpg  \n",
       "\n",
       "[9420 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid = pd.read_csv('valid_csv.csv')\n",
    "train = pd.read_csv('train_csv.csv')\n",
    "valid['label'] = valid['label'].astype(str)\n",
    "train['label'] = train['label'].astype(str)\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b8bb47-9f6c-4815-b420-60f7e3d321fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# this creates the type of augmentation that is done! \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m datagen \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#rotation_range=15,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      5\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      7\u001b[0m     shear_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      8\u001b[0m     horizontal_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     zoom_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.20\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#look at aug from group chat \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#okay this method seems WAY better. only ever use aug images\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_generator\u001b[38;5;241m=\u001b[39mdatagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[1;32m     14\u001b[0m dataframe\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m     15\u001b[0m x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m target_size \u001b[38;5;241m=\u001b[39m image_shape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# this creates the type of augmentation that is done! \n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 15,\n",
    "    horizontal_flip = True,\n",
    "    zoom_range = 0.20)\n",
    "#look at aug from group chat \n",
    "\n",
    "#okay this method seems WAY better. only ever use aug images\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = train['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=valid,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = valid['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42ea244-aa84-47b7-b1c4-ed4298ce6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_data = pd.DataFrame({'real_label': np.unique(train_image['label']), \n",
    "                           'keras_label':pd.DataFrame(np.unique(train_image['label'])).index})\n",
    "class_weights = train_image.merge(small_data, left_on = 'label', right_on = 'real_label')\n",
    "class_number = class_weights['keras_label']\n",
    "class_number = class_weights['real_label'].astype(int)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bae1a-886b-4a97-b043-bfd6a88e3568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "416049e2-4547-49fb-999a-fcfdb4640f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(class_number),\n",
    "                                        y = class_number                                                    \n",
    "                                    )\n",
    "\n",
    "class_weights = dict(zip(np.unique(class_number), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4e3af-a22b-43d3-9834-e51721639c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5777f519-030f-4e53-96d8-7ad95ff239b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 15:36:21.403046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-08 15:36:21.948294: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-12-08 15:36:21.948560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38372 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " resnet50 (Functional)          (None, 2048)         23587712    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)      (None, 2048)         21802784    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4096)         0           ['resnet50[0][0]',               \n",
      "                                                                  'inception_v3[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2097664     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 512)         2048        ['dense[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 200)          102600      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,592,808\n",
      "Trainable params: 5,618,056\n",
      "Non-trainable params: 41,974,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Create the base models (ResNet50 and InceptionV3)\n",
    "base_model_ResNet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "base_model_InceptionV3 = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze most layers but unfreeze the last ones for fine-tuning\n",
    "for layer in base_model_ResNet50.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model_InceptionV3.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the input layer\n",
    "inputs = tf.keras.Input(shape=image_shape_full)\n",
    "\n",
    "# Pass the input through the base models\n",
    "x1 = base_model_ResNet50(inputs, training=False)\n",
    "x2 = base_model_InceptionV3(inputs, training=False)\n",
    "\n",
    "# Merge the features from both base models\n",
    "combined_features = tf.keras.layers.Concatenate()([x1, x2])\n",
    "\n",
    "# Add dense layers with dropout and batch normalization\n",
    "x = tf.keras.layers.Dense(Neurons, activation=\"relu\")(combined_features)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = tf.keras.layers.Dense(output_Neurons, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Adam optimizer\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "\n",
    "# Compile the model \n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb07aea2-099c-4e6c-b825-135f04753f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "295/295 [==============================] - 124s 402ms/step - loss: 3.1410 - accuracy: 0.2261 - val_loss: 3.4856 - val_accuracy: 0.1777\n",
      "Epoch 2/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 3.0653 - accuracy: 0.2380 - val_loss: 3.6740 - val_accuracy: 0.1658\n",
      "Epoch 3/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 2.7818 - accuracy: 0.2898 - val_loss: 3.2678 - val_accuracy: 0.2307\n",
      "Epoch 4/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 2.5789 - accuracy: 0.3340 - val_loss: 3.3490 - val_accuracy: 0.2248\n",
      "Epoch 5/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 2.4370 - accuracy: 0.3588 - val_loss: 2.9730 - val_accuracy: 0.2816\n",
      "Epoch 6/20\n",
      "295/295 [==============================] - 118s 399ms/step - loss: 2.2775 - accuracy: 0.3929 - val_loss: 3.1324 - val_accuracy: 0.2511\n",
      "Epoch 7/20\n",
      "295/295 [==============================] - 119s 404ms/step - loss: 2.1616 - accuracy: 0.4228 - val_loss: 3.1778 - val_accuracy: 0.2485\n",
      "Epoch 8/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 2.0813 - accuracy: 0.4409 - val_loss: 3.0062 - val_accuracy: 0.2765\n",
      "Epoch 9/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 1.9315 - accuracy: 0.4766 - val_loss: 2.8677 - val_accuracy: 0.3113\n",
      "Epoch 10/20\n",
      "295/295 [==============================] - 118s 398ms/step - loss: 1.8223 - accuracy: 0.4947 - val_loss: 3.1748 - val_accuracy: 0.2799\n",
      "Epoch 11/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 1.7056 - accuracy: 0.5329 - val_loss: 3.1060 - val_accuracy: 0.2969\n",
      "Epoch 12/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 1.7369 - accuracy: 0.5316 - val_loss: 3.0880 - val_accuracy: 0.3003\n",
      "Epoch 13/20\n",
      "295/295 [==============================] - 119s 402ms/step - loss: 1.5526 - accuracy: 0.5689 - val_loss: 3.4087 - val_accuracy: 0.2829\n",
      "Epoch 14/20\n",
      "295/295 [==============================] - 117s 398ms/step - loss: 1.4746 - accuracy: 0.5883 - val_loss: 3.0867 - val_accuracy: 0.2969\n",
      "Epoch 15/20\n",
      "295/295 [==============================] - 118s 401ms/step - loss: 1.3796 - accuracy: 0.6148 - val_loss: 2.9966 - val_accuracy: 0.3363\n",
      "Epoch 16/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 1.3000 - accuracy: 0.6304 - val_loss: 3.3282 - val_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 1.2153 - accuracy: 0.6538 - val_loss: 3.2133 - val_accuracy: 0.3142\n",
      "Epoch 18/20\n",
      "295/295 [==============================] - 118s 399ms/step - loss: 1.1974 - accuracy: 0.6582 - val_loss: 3.2767 - val_accuracy: 0.3206\n",
      "Epoch 19/20\n",
      "295/295 [==============================] - 119s 403ms/step - loss: 1.1071 - accuracy: 0.6777 - val_loss: 3.2745 - val_accuracy: 0.3265\n",
      "Epoch 20/20\n",
      "295/295 [==============================] - 118s 400ms/step - loss: 1.0775 - accuracy: 0.6890 - val_loss: 3.2400 - val_accuracy: 0.3261\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# this is supposed to help with over-fitting\n",
    "#Setting the early_stop to avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=patience1,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs= number_epcho1,\n",
    "    batch_size = 52,\n",
    "   # steps_per_epoch=len(train_generator),\n",
    "    validation_data=valid_generator,\n",
    "    #validation_steps=int(0.2 * len(valid_generator)),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    #class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ce402a-c6c2-4e0a-9f4d-137161dc61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('result.png')\n",
    "plt.clf() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cdccb1-5cc7-4bbb-81dd-d8c2a98630e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " resnet50 (Functional)          (None, 2048)         23587712    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)      (None, 2048)         21802784    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4096)         0           ['resnet50[0][0]',               \n",
      "                                                                  'inception_v3[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2097664     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 512)         2048        ['dense[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 200)          102600      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,592,808\n",
      "Trainable params: 5,618,056\n",
      "Non-trainable params: 41,974,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f154a81-fc5e-4672-bcdc-fbab118a1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/60\n",
      "295/295 [==============================] - 115s 353ms/step - loss: 0.8683 - accuracy: 0.7512 - val_loss: 2.9962 - val_accuracy: 0.3733\n",
      "Epoch 28/60\n",
      "295/295 [==============================] - 102s 344ms/step - loss: 0.8253 - accuracy: 0.7651 - val_loss: 2.8628 - val_accuracy: 0.3472\n",
      "Epoch 29/60\n",
      "295/295 [==============================] - 101s 343ms/step - loss: 0.8138 - accuracy: 0.7644 - val_loss: 3.1422 - val_accuracy: 0.3559\n",
      "Epoch 30/60\n",
      "295/295 [==============================] - 101s 342ms/step - loss: 0.7927 - accuracy: 0.7721 - val_loss: 3.0296 - val_accuracy: 0.3403\n",
      "Epoch 31/60\n",
      " 23/295 [=>............................] - ETA: 1:27 - loss: 0.7926 - accuracy: 0.7622"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     14\u001b[0m     patience\u001b[38;5;241m=\u001b[39mpatience2\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m     15\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m     16\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     18\u001b[0m total_epochs \u001b[38;5;241m=\u001b[39m number_epcho1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m---> 20\u001b[0m history_fine \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                         \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# class_weight=class_weights\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2022/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 80\n",
    "# Now that we will allow some layers to be unfreezed, it's better to decrease the learning rate to avoid dramatic changes in those \n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/100)\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(\n",
    "    #loss = \"sparse_categorical_crossentropy\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=patience2+7,\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True,)\n",
    "\n",
    "total_epochs = number_epcho1 + 50\n",
    "\n",
    "history_fine = model.fit(\n",
    "                        train_generator,\n",
    "                         epochs=total_epochs,\n",
    "                         batch_size = 32,\n",
    "                         initial_epoch=26,\n",
    "                         steps_per_epoch=len(train_generator),\n",
    "                         validation_data=valid_generator,\n",
    "                         validation_steps=int(0.25 * len(valid_generator)),\n",
    "                        callbacks=[early_stop, reduce_lr],\n",
    "   # class_weight=class_weights\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339a678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71b72e-2c58-43e0-a77c-1873558068f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6096899-bb39-476c-a66d-6516687e4d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4a266-715a-4bd0-b6f5-9708bd635623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history_fine.history['accuracy'])\n",
    "plt.plot(history_fine.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37765a38-cbcc-4ae5-bc40-b3fdfc22184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = \"feather-in-focus\"\n",
    "model.save('{}/keras_final.keras'.format(current_directory))\n",
    "\n",
    "# image folder\n",
    "folder_path = '{}/test_images/test_images/'.format(current_directory)\n",
    "# path to model\n",
    "model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "# dimensions of images\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09f44c-5270-4428-aa43-03f0a9b6726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "# load the trained model\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "model = load_model(model_path)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bfb8c1a-3551-4bb8-9440-2a311384c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list()\n",
    "images = []\n",
    "folder_path = 'feather-in-focus/test_images/test_images'  # Your folder path containing images\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    # Check if the item is a file (not a directory)\n",
    "    if os.path.isfile(img_path):\n",
    "        name.append(img_name)\n",
    "        img = image.load_img(img_path, target_size=image_shape)\n",
    "        img = image.img_to_array(img)\n",
    "        \n",
    "        \n",
    "        #img_array = image.img_to_array(img)\n",
    "        #img = img.astype('float32') / 255.0  # Normalize pixel values\n",
    "\n",
    "# Reshape the image to a 4D tensor with a single sample (to match model input shape)\n",
    "        #img = img.reshape((1,) + img.shape)\n",
    "        \n",
    "        \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "        # Process the image further as needed\n",
    "    else:\n",
    "        # Skip directories\n",
    "        continue\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d812d7f-e48e-473d-9aa4-38697b8edc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0ea6649-cac1-4b50-ac15-8f755db542ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 10s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = model.predict(images, batch_size=10)\n",
    "classes_x=np.argmax(classes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0311fc31-09b1-4c2c-a8cd-0a740a4a1ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2472</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3848</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3401</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3540</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3510</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3093</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>352</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2812</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label\n",
       "0     2472     99\n",
       "1     3848      5\n",
       "2     2025     20\n",
       "3      910    119\n",
       "4     3401    176\n",
       "...    ...    ...\n",
       "3995  3540    129\n",
       "3996  3510     32\n",
       "3997  3093     48\n",
       "3998   352     46\n",
       "3999  2812     25\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(\n",
    "    {'id': name,\n",
    "     'label_predict': classes_x\n",
    "    })\n",
    "\n",
    "final_df['id2'] = \"/test_images/\"+final_df['id']\n",
    "final_df['label_predict'] = final_df['label_predict'].astype(int)\n",
    "final_df = final_df[['label_predict','id2']]\n",
    "\n",
    "idea = np.sort(train_image['label'].unique())\n",
    "idea2 = pd.DataFrame(idea)\n",
    "idea2['index1'] = idea2.index\n",
    "idea2.columns = ['values', 'index1']\n",
    "idea2['values'] = idea2['values'].astype(int)\n",
    "\n",
    "final_df4 = final_df.merge(test_image_path[['image_path','id']], left_on = 'id2', right_on='image_path')\n",
    "\n",
    "final_df4 = final_df4[['id','label_predict']]\n",
    "\n",
    "final_df2 = final_df4.merge(idea2, how = 'left', left_on = 'label_predict', right_on = 'index1')\n",
    "final_df3 = final_df2[['id','values']]\n",
    "#two issues the class values arent right\n",
    "final_df3.columns = ['id','label']\n",
    "final_df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70a672dc-cc4a-4290-b23e-674f2771c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df3.to_csv('output_folder/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c943d-aa26-497a-b061-90c6089afd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the metadata for your dataset\n",
    "metadata = {\n",
    "    \"title\": \"output.csv\",\n",
    "    \"id\": \"uvajonathon\",\n",
    "    \"licenses\": [\n",
    "        {\n",
    "            \"name\": \"License Name\",\n",
    "            \"id\": \"uvajonathon\",\n",
    "            \"url\": \"URL to License if applicable\"\n",
    "        }\n",
    "    ],\n",
    "    \"description\": \"Description of your dataset\",\n",
    "    \"keywords\": [\"keyword1\", \"keyword2\"],\n",
    "    \"language\": \"en\",\n",
    "    \"isPrivate\": False,  # Set to True or False based on privacy\n",
    "    \"collaborators\": [\"uvajonathon\"],\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"description\": \"Description of the data\",\n",
    "            \"name\": \"output.csv\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the metadata to dataset-metadata.json\n",
    "file_path = 'output_folder/dataset-metadata.json'  # Specify the path where you want to save the file\n",
    "\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Dataset metadata saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b923057-24fc-4014-bbde-884e2411a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# Set the path to your Kaggle API credentials file (kaggle.json)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/gpfs/home2/scur2079/'\n",
    "# Instantiate the Kaggle API\n",
    "api = KaggleApi()\n",
    "# Replace 'competition-name' with the competition name (as shown in the competition URL)\n",
    "competition_name = 'feather-in-focus'\n",
    "# Replace 'file-path.csv' with the path to your CSV file\n",
    "file_path = '/gpfs/home2/scur2079/output_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ba3c9-b563-4074-8edd-5286f9103546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the competition to which you want to upload the file\n",
    "api.competition_name = competition_name\n",
    "# Upload the CSV file to the competition\n",
    "api.dataset_create_new(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1c890d-a845-434b-9afd-33e921d88e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x155294da8370>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b78d1a-9fdc-4412-948b-68de182a2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 15:14:52.610143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/197 [..............................] - ETA: 17:14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 15:14:54.857331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 70s 331ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train your Keras model as described in your provided code\n",
    "\n",
    "# Assuming you've trained your Keras model and stored it in a variable 'model'\n",
    "from keras.models import Model\n",
    "# Extract features using the Keras Functional API\n",
    "# Create a new model that takes the same input as the previous model but outputs the output of the desired layer\n",
    "layer_name='output-layer'\n",
    "feature_extraction_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "# \n",
    "# Get features for training and validation data\n",
    "train_features = feature_extraction_model.predict(train_generator)  # Replace train_data with your actual training data\n",
    "#valid_features = feature_extraction_model.predict(valid)  # Replace valid_data with your actual validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1072233a-0e01-4de0-acb0-b52769386e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.image.DataFrameIterator"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3841d9b1-1acb-4592-9fcf-86c39b74cfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_labels() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels_batch \u001b[38;5;129;01min\u001b[39;00m train_generator:\n\u001b[0;32m----> 7\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_labels() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "source": [
    "def extract_labels(image, label):\n",
    "    return label\n",
    "\n",
    "# Iterate through the generator to extract labels\n",
    "labels = []\n",
    "for images, labels_batch in train_generator:\n",
    "    labels.extend(extract_labels(images, labels_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cd146b7-8457-4984-a4a9-0282f9af33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22dfd7fd-0162-494a-b51f-83bb2e8d5aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0], got [dict_keys(['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'])]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()  \u001b[38;5;66;03m# You can set hyperparameters as needed\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:1467\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1464\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1466\u001b[0m ):\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1470\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0], got [dict_keys(['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'])]"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()  # You can set hyperparameters as needed\n",
    "xgb_model.fit(train_features,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1238ea9e-6c0b-4f97-a793-49c57341cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from xgboost) (1.22.3)\n",
      "Requirement already satisfied: scipy in /gpfs/admin/_hpc/sw/arch/INTEL-AVX512/RHEL8/EB_production/2022/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from xgboost) (1.8.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/sw/arch/RHEL8/EB_production/2022/software/Python/3.10.4-GCCcore-11.3.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da52c494-620a-4e15-8d10-ea63c50687e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check if the item is a file (not a directory)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(img_path):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mname\u001b[49m\u001b[38;5;241m.\u001b[39mappend(img_name)\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mload_img(img_path, target_size\u001b[38;5;241m=\u001b[39mimage_shape)\n\u001b[1;32m      9\u001b[0m     img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "name = list()\n",
    "images = []\n",
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "folder_path = 'feather-in-focus/test_images/test_images'  # Your folder path containing images\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    # Check if the item is a file (not a directory)\n",
    "    if os.path.isfile(img_path):\n",
    "        name.append(img_name)\n",
    "        img = image.load_img(img_path, target_size=image_shape)\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "        # Process the image further as needed\n",
    "    else:\n",
    "        # Skip directories\n",
    "        continue\n",
    "        \n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = feature_extraction_model.predict(images, batch_size=10)\n",
    "classes_x=np.argmax(classes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff49b730-4777-4244-8922-1ce619171d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "valid_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "483c99bb-1486-45ea-93b4-9641014fe3cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m classes_x\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(valid_features,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m----> 3\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mname\u001b[49m,\n\u001b[1;32m      4\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_predict\u001b[39m\u001b[38;5;124m'\u001b[39m: classes_x\n\u001b[1;32m      5\u001b[0m     })\n\u001b[1;32m      7\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/test_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfinal_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_predict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_predict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "classes_x=np.argmax(valid_features,axis=1)\n",
    "final_df = pd.DataFrame(\n",
    "    {'id': name,\n",
    "     'label_predict': classes_x\n",
    "    })\n",
    "\n",
    "final_df['id2'] = \"/test_images/\"+final_df['id']\n",
    "final_df['label_predict'] = final_df['label_predict'].astype(int)\n",
    "final_df = final_df[['label_predict','id2']]\n",
    "\n",
    "idea = np.sort(train_image['label'].unique())\n",
    "idea2 = pd.DataFrame(idea)\n",
    "idea2['index1'] = idea2.index\n",
    "idea2.columns = ['values', 'index1']\n",
    "idea2['values'] = idea2['values'].astype(int)\n",
    "\n",
    "final_df4 = final_df.merge(test_image_path[['image_path','id']], left_on = 'id2', right_on='image_path')\n",
    "\n",
    "final_df4 = final_df4[['id','label_predict']]\n",
    "\n",
    "final_df2 = final_df4.merge(idea2, how = 'left', left_on = 'label_predict', right_on = 'index1')\n",
    "final_df3 = final_df2[['id','values']]\n",
    "#two issues the class values arent right\n",
    "final_df3.columns = ['id','label']\n",
    "final_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fc304-f43f-4c3a-b75f-445051a1ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
