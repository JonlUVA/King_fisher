{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55ff5a-5e53-42c2-97e9-0114d3713916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabb601c-5990-42cc-a2ba-ef7eb5c4c5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untitled.ipynb',\n",
       " '.jupyter',\n",
       " '.config',\n",
       " 'untitled.py',\n",
       " 'jupyterhub-2022-4581118.out',\n",
       " '.local',\n",
       " '.ipynb_checkpoints',\n",
       " 'jupyterhub-2022-4581898.out',\n",
       " 'jupyterhub-2022-4581898.error',\n",
       " 'Untitled1.ipynb',\n",
       " '.keras',\n",
       " 'feather-in-focus.zip',\n",
       " '.jupyter-2022',\n",
       " '.ipython',\n",
       " '.bash_profile',\n",
       " '.cache',\n",
       " '.bashrc',\n",
       " 'jupyterhub-2022-4581118.error']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import pandas\n",
    " \n",
    "od.download(\"https://www.kaggle.com/competitions/feather-in-focus/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbef826-114f-41b5-95d1-5b877b5e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/chekoduadarsh/starters-guide-convolutional-xgboost/notebook\n",
    "#https://www.kaggle.com/code/pedrolucasbritodes/bird-image-classification-cnn-89-accuracy/notebook\n",
    "#https://stats.stackexchange.com/questions/404809/is-it-advisable-to-use-output-from-a-ml-model-as-a-feature-in-another-ml-model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "#how many epchos still revert\n",
    "patience1 = 3\n",
    "\n",
    "base_learning_rate = 0.001 \n",
    "#number before fine_tuning\n",
    "number_epcho1 = 20\n",
    "#number after fine_tuning\n",
    "number_epcho2 = 20\n",
    "number_class = 200\n",
    "#needs to be the same number of classes\n",
    "output_Neurons = 200\n",
    "Neurons = 512\n",
    "drop_out_rate = 0.3\n",
    "image_shape = (224,224)\n",
    "image_shape_full = (224,224,3)\n",
    "current_directory = \"feather-in-focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1ccf2b-4ac4-4cae-aac6-0b88a172b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2079.4584660/ipykernel_1454364/3190752840.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
      "/scratch-local/scur2079.4584660/ipykernel_1454364/3190752840.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>total_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/train_images/1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/train_images/2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/train_images/3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/train_images/4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/train_images/5.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>feather-in-focus/train_images/train_images/5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_path label                                        total_path\n",
       "0  /train_images/1.jpg     1  feather-in-focus/train_images/train_images/1.jpg\n",
       "1  /train_images/2.jpg     1  feather-in-focus/train_images/train_images/2.jpg\n",
       "2  /train_images/3.jpg     1  feather-in-focus/train_images/train_images/3.jpg\n",
       "3  /train_images/4.jpg     1  feather-in-focus/train_images/train_images/4.jpg\n",
       "4  /train_images/5.jpg     1  feather-in-focus/train_images/train_images/5.jpg"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading files \n",
    "class_names2 = np.load('feather-in-focus/class_names.npy', allow_pickle=True).item()\n",
    "\n",
    "#load all files because why not\n",
    "test_image_path = pd.read_csv('feather-in-focus/test_images_path.csv')\n",
    "test_image_path['total_path'] = current_directory + \"/test_images\" + test_image_path['image_path'] \n",
    "test_image_path['total_path'] = test_image_path['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "test_image = pd.read_csv('feather-in-focus/test_images_sample.csv')\n",
    "\n",
    "#importing training data \n",
    "train_image = pd.read_csv('feather-in-focus/train_images.csv')\n",
    "train_image['total_path'] = current_directory + \"/train_images\" + train_image['image_path']\n",
    "train_image['total_path'] = train_image['total_path'].str.replace(\"\\\\\", \"/\" )\n",
    "train_image['label'] = train_image['label'].apply(str)\n",
    "train_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c216ed-d088-45d8-a6f6-0d3e8f125e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add a k-folds, i have stratified so it works a bit better\n",
    "train, valid = train_test_split(train_image, \n",
    "                    test_size=0.2,\n",
    "                    stratify = train_image['label'],\n",
    "                    random_state=420)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b8bb47-9f6c-4815-b420-60f7e3d321fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3140 validated image filenames belonging to 200 classes.\n",
      "Found 786 validated image filenames belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "# this creates the type of augmentation that is done! \n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 15,\n",
    "    horizontal_flip = True,\n",
    "    zoom_range = 0.20)\n",
    "#look at aug from group chat \n",
    "\n",
    "#okay this method seems WAY better. only ever use aug images\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = train['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=valid,\n",
    "x_col='total_path',\n",
    "y_col='label',\n",
    "#classes = valid['label'].tolist(),\n",
    "color_mode = 'rgb',\n",
    "target_size = image_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499ffce8-bf3d-436b-a197-2336a0e1a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 17:52:17.782805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 17:52:18.306242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18209 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:31:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " output-layer (Dense)        (None, 200)               102600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,006,152\n",
      "Trainable params: 1,416,392\n",
      "Non-trainable params: 23,589,760\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 17:52:28.253171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-12-04 17:52:31.072678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-04 17:52:31.117894: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x26e160a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-04 17:52:31.117922: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0\n",
      "2023-12-04 17:52:31.140635: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-04 17:52:31.372715: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 51s 412ms/step - loss: 5.2042 - accuracy: 0.0481 - val_loss: 4.6907 - val_accuracy: 0.0625\n",
      "Epoch 2/20\n",
      "99/99 [==============================] - 36s 371ms/step - loss: 3.7978 - accuracy: 0.1583 - val_loss: 3.8277 - val_accuracy: 0.1615\n",
      "Epoch 3/20\n",
      "99/99 [==============================] - 36s 366ms/step - loss: 3.2513 - accuracy: 0.2516 - val_loss: 3.2692 - val_accuracy: 0.2552\n",
      "Epoch 4/20\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.9222 - accuracy: 0.2978 - val_loss: 3.4287 - val_accuracy: 0.2135\n",
      "Epoch 5/20\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.6484 - accuracy: 0.3417 - val_loss: 3.5354 - val_accuracy: 0.2344\n",
      "Epoch 6/20\n",
      "99/99 [==============================] - 36s 366ms/step - loss: 2.4533 - accuracy: 0.3720 - val_loss: 3.3305 - val_accuracy: 0.2292\n",
      "Epoch 6/40\n",
      "99/99 [==============================] - 43s 380ms/step - loss: 2.7698 - accuracy: 0.3347 - val_loss: 3.3273 - val_accuracy: 0.2448\n",
      "Epoch 7/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.7268 - accuracy: 0.3376 - val_loss: 3.4320 - val_accuracy: 0.1927\n",
      "Epoch 8/40\n",
      "99/99 [==============================] - 36s 366ms/step - loss: 2.7037 - accuracy: 0.3271 - val_loss: 3.2948 - val_accuracy: 0.2708\n",
      "Epoch 9/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6723 - accuracy: 0.3414 - val_loss: 3.0772 - val_accuracy: 0.3177\n",
      "Epoch 10/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6416 - accuracy: 0.3573 - val_loss: 2.9729 - val_accuracy: 0.2812\n",
      "Epoch 11/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6448 - accuracy: 0.3443 - val_loss: 3.0012 - val_accuracy: 0.3125\n",
      "Epoch 12/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6466 - accuracy: 0.3503 - val_loss: 3.0515 - val_accuracy: 0.2760\n",
      "Epoch 13/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6303 - accuracy: 0.3443 - val_loss: 3.0801 - val_accuracy: 0.2552\n",
      "Epoch 14/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.6378 - accuracy: 0.3510 - val_loss: 3.1740 - val_accuracy: 0.2812\n",
      "Epoch 15/40\n",
      "99/99 [==============================] - 36s 367ms/step - loss: 2.5693 - accuracy: 0.3682 - val_loss: 3.0204 - val_accuracy: 0.2760\n",
      "Epoch 16/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.5602 - accuracy: 0.3701 - val_loss: 3.0419 - val_accuracy: 0.3073\n",
      "Epoch 17/40\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.6071 - accuracy: 0.3449 - val_loss: 2.9434 - val_accuracy: 0.2760\n",
      "Epoch 18/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.5417 - accuracy: 0.3844 - val_loss: 3.1522 - val_accuracy: 0.2812\n",
      "Epoch 19/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.5545 - accuracy: 0.3627 - val_loss: 3.0430 - val_accuracy: 0.2812\n",
      "Epoch 20/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.5126 - accuracy: 0.3761 - val_loss: 3.0171 - val_accuracy: 0.3073\n",
      "Epoch 21/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.5055 - accuracy: 0.3911 - val_loss: 3.0641 - val_accuracy: 0.2760\n",
      "Epoch 22/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.5394 - accuracy: 0.3615 - val_loss: 3.1326 - val_accuracy: 0.2448\n",
      "Epoch 23/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.4787 - accuracy: 0.3761 - val_loss: 2.7043 - val_accuracy: 0.3438\n",
      "Epoch 24/40\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.4959 - accuracy: 0.3780 - val_loss: 3.0551 - val_accuracy: 0.2188\n",
      "Epoch 25/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.4916 - accuracy: 0.3748 - val_loss: 3.0142 - val_accuracy: 0.3021\n",
      "Epoch 26/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.5050 - accuracy: 0.3815 - val_loss: 2.9777 - val_accuracy: 0.3073\n",
      "Epoch 27/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.4994 - accuracy: 0.3799 - val_loss: 3.0706 - val_accuracy: 0.3021\n",
      "Epoch 28/40\n",
      "99/99 [==============================] - 36s 367ms/step - loss: 2.4482 - accuracy: 0.3904 - val_loss: 3.1677 - val_accuracy: 0.2292\n",
      "Epoch 29/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.4554 - accuracy: 0.3885 - val_loss: 3.0881 - val_accuracy: 0.2448\n",
      "Epoch 30/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.4289 - accuracy: 0.3946 - val_loss: 2.9081 - val_accuracy: 0.3177\n",
      "Epoch 31/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3944 - accuracy: 0.3997 - val_loss: 3.0115 - val_accuracy: 0.2812\n",
      "Epoch 32/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.4308 - accuracy: 0.4054 - val_loss: 2.9715 - val_accuracy: 0.3073\n",
      "Epoch 33/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.4033 - accuracy: 0.4016 - val_loss: 3.0295 - val_accuracy: 0.2760\n",
      "Epoch 34/40\n",
      "99/99 [==============================] - 37s 377ms/step - loss: 2.3865 - accuracy: 0.3971 - val_loss: 3.0014 - val_accuracy: 0.3333\n",
      "Epoch 35/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.4094 - accuracy: 0.4000 - val_loss: 2.9261 - val_accuracy: 0.2865\n",
      "Epoch 36/40\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.3791 - accuracy: 0.4022 - val_loss: 3.0522 - val_accuracy: 0.2656\n",
      "Epoch 37/40\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.4025 - accuracy: 0.4029 - val_loss: 2.9342 - val_accuracy: 0.3073\n",
      "Epoch 38/40\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.4047 - accuracy: 0.3917 - val_loss: 2.9439 - val_accuracy: 0.3229\n",
      "Epoch 39/40\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.3685 - accuracy: 0.4070 - val_loss: 2.8068 - val_accuracy: 0.3073\n",
      "Epoch 40/40\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.3718 - accuracy: 0.4127 - val_loss: 3.0361 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# creating the model \n",
    "# changing the base model to resnet as requested\n",
    "\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape_full,\n",
    "    pooling='avg',\n",
    "    classes=number_class,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the ResNet50 model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a Sequential model\n",
    "# this model has two models in it, we just need to make ours.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add the base model (ResNET) to the Sequential model\n",
    "model.add(base_model)\n",
    "\n",
    "# Global Average Pooling layer\n",
    "# this layer doesnt work with resnet\n",
    "#model.add(tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\"))\n",
    "#model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "#More hidden layers, now with BatchNormalization\n",
    "model.add(tf.keras.layers.Dense(Neurons, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(drop_out_rate))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(Neurons, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(drop_out_rate))\n",
    "# Output Dense layer\n",
    "model.add(tf.keras.layers.Dense(output_Neurons, activation=\"softmax\", name=\"output-layer\"))\n",
    "\n",
    "\n",
    "# adam optim is apparently the best\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "\n",
    "# Compile the model \n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    #loss = \"sparse_categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# this is supposed to help with over-fitting\n",
    "#Setting the early_stop to avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=patience1,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs= number_epcho1,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=int(0.25 * len(valid_generator)),\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd4a266-715a-4bd0-b6f5-9708bd635623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "99/99 [==============================] - 43s 377ms/step - loss: 2.3436 - accuracy: 0.4096 - val_loss: 3.2635 - val_accuracy: 0.2812\n",
      "Epoch 7/30\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.3443 - accuracy: 0.4175 - val_loss: 2.9556 - val_accuracy: 0.2917\n",
      "Epoch 8/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3611 - accuracy: 0.4137 - val_loss: 3.1611 - val_accuracy: 0.2396\n",
      "Epoch 9/30\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.3520 - accuracy: 0.4061 - val_loss: 2.9015 - val_accuracy: 0.2917\n",
      "Epoch 10/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3532 - accuracy: 0.4108 - val_loss: 3.1145 - val_accuracy: 0.2448\n",
      "Epoch 11/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3833 - accuracy: 0.3920 - val_loss: 2.9748 - val_accuracy: 0.2917\n",
      "Epoch 12/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3391 - accuracy: 0.4220 - val_loss: 2.7182 - val_accuracy: 0.3698\n",
      "Epoch 13/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.3483 - accuracy: 0.4096 - val_loss: 2.9037 - val_accuracy: 0.2865\n",
      "Epoch 14/30\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.2901 - accuracy: 0.4232 - val_loss: 2.9569 - val_accuracy: 0.2865\n",
      "Epoch 15/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.3117 - accuracy: 0.4169 - val_loss: 3.0757 - val_accuracy: 0.2708\n",
      "Epoch 16/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.2968 - accuracy: 0.4201 - val_loss: 2.7588 - val_accuracy: 0.3385\n",
      "Epoch 17/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.3224 - accuracy: 0.4127 - val_loss: 2.9017 - val_accuracy: 0.3438\n",
      "Epoch 18/30\n",
      "99/99 [==============================] - 36s 367ms/step - loss: 2.2662 - accuracy: 0.4283 - val_loss: 3.1063 - val_accuracy: 0.2708\n",
      "Epoch 19/30\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 2.2891 - accuracy: 0.4283 - val_loss: 2.8699 - val_accuracy: 0.3073\n",
      "Epoch 20/30\n",
      "99/99 [==============================] - 36s 368ms/step - loss: 2.2539 - accuracy: 0.4471 - val_loss: 3.0405 - val_accuracy: 0.3073\n",
      "Epoch 21/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2726 - accuracy: 0.4271 - val_loss: 2.9136 - val_accuracy: 0.2812\n",
      "Epoch 22/30\n",
      "99/99 [==============================] - 36s 366ms/step - loss: 2.2431 - accuracy: 0.4204 - val_loss: 3.0390 - val_accuracy: 0.3021\n",
      "Epoch 23/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2522 - accuracy: 0.4363 - val_loss: 3.0546 - val_accuracy: 0.2708\n",
      "Epoch 24/30\n",
      "99/99 [==============================] - 37s 375ms/step - loss: 2.2361 - accuracy: 0.4334 - val_loss: 3.0493 - val_accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.2216 - accuracy: 0.4404 - val_loss: 2.9342 - val_accuracy: 0.2917\n",
      "Epoch 26/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2639 - accuracy: 0.4299 - val_loss: 2.9522 - val_accuracy: 0.2969\n",
      "Epoch 27/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2470 - accuracy: 0.4261 - val_loss: 2.9603 - val_accuracy: 0.2917\n",
      "Epoch 28/30\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 2.2256 - accuracy: 0.4373 - val_loss: 3.0121 - val_accuracy: 0.2969\n",
      "Epoch 29/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2094 - accuracy: 0.4420 - val_loss: 2.9527 - val_accuracy: 0.3073\n",
      "Epoch 30/30\n",
      "99/99 [==============================] - 36s 364ms/step - loss: 2.2340 - accuracy: 0.4271 - val_loss: 2.9737 - val_accuracy: 0.2917\n"
     ]
    }
   ],
   "source": [
    "#number of base model layers\n",
    "fine_tune_at = 176\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Now that we will allow some layers to be unfreezed, it's better to decrease the learning rate to avoid dramatic changes in those \n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/100)\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(\n",
    "    #loss = \"sparse_categorical_crossentropy\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=4,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,)\n",
    "\n",
    "\n",
    "total_epochs = number_epcho1 + 10\n",
    "\n",
    "history_fine = model.fit(\n",
    "                        train_generator,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         steps_per_epoch=len(train_generator),\n",
    "                         validation_data=valid_generator,\n",
    "                         validation_steps=int(0.25 * len(valid_generator)),\n",
    "                        #callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37765a38-cbcc-4ae5-bc40-b3fdfc22184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "current_directory = \"feather-in-focus\"\n",
    "model.save('{}/keras_final.keras'.format(current_directory))\n",
    "\n",
    "\n",
    "# image folder\n",
    "folder_path = '{}/test_images/test_images/'.format(current_directory)\n",
    "# path to model\n",
    "model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "# dimensions of images\n",
    "img_width, img_height = 224, 224\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a09f44c-5270-4428-aa43-03f0a9b6726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:03:14.433714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 19:03:14.809359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18209 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:31:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import keras.utils as image\n",
    "# load the trained model\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model_path = '{}/keras_final.keras'.format(current_directory)\n",
    "model = load_model(model_path)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfb8c1a-3551-4bb8-9440-2a311384c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:03:38.495576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/400 [>.............................] - ETA: 4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:03:40.866858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 10s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "name = list()\n",
    "images = []\n",
    "folder_path = 'feather-in-focus/test_images/test_images'  # Your folder path containing images\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    # Check if the item is a file (not a directory)\n",
    "    if os.path.isfile(img_path):\n",
    "        name.append(img_name)\n",
    "        img = image.load_img(img_path, target_size=image_shape)\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "        # Process the image further as needed\n",
    "    else:\n",
    "        # Skip directories\n",
    "        continue\n",
    "        \n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = model.predict(images, batch_size=10)\n",
    "classes_x=np.argmax(classes,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0311fc31-09b1-4c2c-a8cd-0a740a4a1ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2472</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3848</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3401</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3540</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3510</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3093</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>352</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2812</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label\n",
       "0     2472    117\n",
       "1     3848      7\n",
       "2     2025     47\n",
       "3      910    119\n",
       "4     3401     85\n",
       "...    ...    ...\n",
       "3995  3540    132\n",
       "3996  3510     33\n",
       "3997  3093     48\n",
       "3998   352     46\n",
       "3999  2812     25\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(\n",
    "    {'id': name,\n",
    "     'label_predict': classes_x\n",
    "    })\n",
    "\n",
    "final_df['id2'] = \"/test_images/\"+final_df['id']\n",
    "final_df['label_predict'] = final_df['label_predict'].astype(int)\n",
    "final_df = final_df[['label_predict','id2']]\n",
    "\n",
    "idea = np.sort(train_image['label'].unique())\n",
    "idea2 = pd.DataFrame(idea)\n",
    "idea2['index1'] = idea2.index\n",
    "idea2.columns = ['values', 'index1']\n",
    "idea2['values'] = idea2['values'].astype(int)\n",
    "\n",
    "final_df4 = final_df.merge(test_image_path[['image_path','id']], left_on = 'id2', right_on='image_path')\n",
    "\n",
    "final_df4 = final_df4[['id','label_predict']]\n",
    "\n",
    "final_df2 = final_df4.merge(idea2, how = 'left', left_on = 'label_predict', right_on = 'index1')\n",
    "final_df3 = final_df2[['id','values']]\n",
    "#two issues the class values arent right\n",
    "final_df3.columns = ['id','label']\n",
    "final_df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70a672dc-cc4a-4290-b23e-674f2771c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df3.to_csv('output_folder/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b923057-24fc-4014-bbde-884e2411a0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata file not found: dataset-metadata.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m api\u001b[38;5;241m.\u001b[39mcompetition_name \u001b[38;5;241m=\u001b[39m competition_name\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Upload the CSV file to the competition\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_create_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py:1720\u001b[0m, in \u001b[0;36mKaggleApi.dataset_create_new\u001b[0;34m(self, folder, public, quiet, convert_to_csv, dir_mode)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder):\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid folder: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m folder)\n\u001b[0;32m-> 1720\u001b[0m meta_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset_metadata_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# read json\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(meta_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py:3483\u001b[0m, in \u001b[0;36mKaggleApi.get_dataset_metadata_file\u001b[0;34m(self, folder)\u001b[0m\n\u001b[1;32m   3481\u001b[0m     meta_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOLD_DATASET_METADATA_FILE)\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(meta_file):\n\u001b[0;32m-> 3483\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata file not found: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   3484\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDATASET_METADATA_FILE)\n\u001b[1;32m   3485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meta_file\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata file not found: dataset-metadata.json"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# Set the path to your Kaggle API credentials file (kaggle.json)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = 'kaggle.json'\n",
    "# Instantiate the Kaggle API\n",
    "api = KaggleApi()\n",
    "# Replace 'competition-name' with the competition name (as shown in the competition URL)\n",
    "competition_name = 'feather-in-focus'\n",
    "# Replace 'file-path.csv' with the path to your CSV file\n",
    "file_path = 'feather-in-focus'\n",
    "# Specify the competition to which you want to upload the file\n",
    "api.competition_name = competition_name\n",
    "# Upload the CSV file to the competition\n",
    "api.dataset_create_new(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b78d1a-9fdc-4412-948b-68de182a2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52c494-620a-4e15-8d10-ea63c50687e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
